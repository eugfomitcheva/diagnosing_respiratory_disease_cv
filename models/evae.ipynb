{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Organizing Data #"
      ],
      "metadata": {
        "id": "O6psw7ckHCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kri_iQHdHRZc",
        "outputId": "955135d1-ff6c-4ff5-8293-933c79e0f392"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't need to re-run"
      ],
      "metadata": {
        "id": "Q1PDJwKzbowB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "# Set paths to image folders\n",
        "class1_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Viral_Pneumonia/images'\n",
        "class2_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Normal/images'\n",
        "class3_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Lung_Opacity/images'\n",
        "class4_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/COVID/images'\n",
        "\n",
        "# Set paths to output directories\n",
        "train_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/val'\n",
        "test_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/test'\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Split images into train, validation, and test sets\n",
        "for class_dir, class_name in zip([class1_dir, class2_dir, class3_dir, class4_dir], ['class1', 'class2', 'class3', 'class4']):\n",
        "    image_files = os.listdir(class_dir)\n",
        "    train_files, test_files = train_test_split(image_files, test_size=0.1, random_state=42)\n",
        "    train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Copy train images to train folder\n",
        "    for file_name in train_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(train_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n",
        "\n",
        "    # Copy validation images to validation folder\n",
        "    for file_name in val_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(val_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n",
        "\n",
        "    # Copy test images to test folder\n",
        "    for file_name in test_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(test_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n"
      ],
      "metadata": {
        "id": "ohY784FXHCJQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzyVsKjhi0v0"
      },
      "source": [
        "# Advanced Models: EVAE-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IL3egfh8i0v3"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.models as models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kAJB8cBKi0v5"
      },
      "outputs": [],
      "source": [
        "class EVAE(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super(EVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes #4\n",
        "        \n",
        "        # Define ResNet50 Encoder\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        resnet_layers = list(resnet.children())[:-1]  # Remove last layer (classification head)\n",
        "        self.resnet_encoder = nn.Sequential(*resnet_layers)\n",
        "        \n",
        "        # Define VGG16 Encoder\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        vgg16_layers = list(vgg16.features.children())[:-1]  # Remove last layer (max pooling)\n",
        "        self.vgg16_encoder = nn.Sequential(*vgg16_layers)\n",
        "\n",
        "        # Define reparameterization layers\n",
        "        self.fc0 = nn.Linear(100864, latent_dim)\n",
        "        self.fc1 = nn.Linear(latent_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, latent_dim*2)\n",
        "\n",
        "        # Define classification head\n",
        "        self.classification_head = nn.Linear(latent_dim, num_classes)\n",
        "        \n",
        "        # Define decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def decode(self, z):\n",
        "        x_hat = self.decoder(z.unsqueeze(-1).unsqueeze(-1))\n",
        "\n",
        "        return x_hat\n",
        "\n",
        "    def encode(self, x):\n",
        "        # encode\n",
        "        resnet_features = self.resnet_encoder(x)\n",
        "        vgg16_features = self.vgg16_encoder(x)\n",
        "\n",
        "        # flatten the features and concatenate them\n",
        "        features = torch.cat([resnet_features.view(x.size(0), -1), \n",
        "                              vgg16_features.view(x.size(0), -1)], dim=1)\n",
        "        \n",
        "        # apply reparameterization\n",
        "        x = F.relu(self.fc0(features))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        mu, log_var = torch.chunk(x, 2, dim=-1)\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "\n",
        "        return z, mu, log_var\n",
        "\n",
        "        # def forward(self, x):\n",
        "    #     mu, log_var = self.encode(x)\n",
        "    #     std = torch.exp(0.5 * log_var)\n",
        "    #     eps = torch.randn_like(std)\n",
        "    #     z = eps * std + mu\n",
        "    #     x_hat = self.decode(z)\n",
        "    #     y = self.classification_head(z)\n",
        "    #     return x_hat, y, mu, log_var\n",
        "\n",
        "    def forward(self, x):\n",
        "          print('x:', x.shape)\n",
        "          z, mu, log_var = self.encode(x)\n",
        "          x_hat = self.decode(z)\n",
        "          y = self.classification_head(z)\n",
        "          print('x_hat:', x_hat.shape)\n",
        "          print('z:', z.shape)\n",
        "          print('y:', y.shape)\n",
        "          return x_hat, y, mu, log_var\n",
        "    \n",
        "    def loss_function(self, x_hat, x, y, target, mu, log_var):\n",
        "        num_pixels = x.shape[1] * x.shape[2] * x.shape[3]\n",
        "        # Upsample to get x and x_hat pixels matching\n",
        "        x_hat_upsampled = F.interpolate(x_hat, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "      \n",
        "        # Compute reconstruction loss\n",
        "        recons_loss = F.mse_loss(x_hat_upsampled, x, reduction='sum')\n",
        "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        # Lcls = F.cross_entropy(torch.argmax(y, dim=1), torch.argmax(target, dim=1))\n",
        "        Lcls = F.cross_entropy(y, target.argmax(dim=1))\n",
        "        \n",
        "        return recons_loss, kld_loss, Lcls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "0wFHZC0odaAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms # need to adapt image format\n",
        "\n",
        "\n",
        "# Access train data\n",
        "train_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/train'\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
        "train_data = datasets.ImageFolder(train_dir, transform=transform)"
      ],
      "metadata": {
        "id": "RWtgOVgm-ovV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "siZyCYhz5v08"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "def train(model, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, y, mu, log_var = model(data)\n",
        "        target_onehot = F.one_hot(target, num_classes=4).float() \n",
        "        BCE, KLD, Lcls = model.loss_function(recon_batch, data, y, target_onehot, mu, log_var)\n",
        "        loss = BCE + KLD + Lcls\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "OV192UynYuQX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = EVAE(latent_dim=256, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(model, optimizer, train_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w6Sz5yncY01h",
        "outputId": "a17a75cf-7c41-4663-dd41-9ca037099fac"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-0.0995, -0.7553, -0.0288,  0.6865],\n",
            "        [ 0.5592,  0.4866, -0.5601,  0.5061],\n",
            "        [-0.0401, -0.0015,  0.3525, -0.1635],\n",
            "        [-0.1565, -0.5974, -0.0158, -0.2127]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [0/14435 (0%)]\tLoss: 119847.312500\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[ 0.2599,  1.7462, -0.2203, -0.4450],\n",
            "        [ 0.7800, -0.5635,  0.3680,  0.2479],\n",
            "        [ 0.4095,  1.1414, -1.1106, -0.9730],\n",
            "        [-0.2493, -0.0679, -0.3846,  1.1143]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "tensor([[ 0.2413, -0.8077,  0.9843,  0.4528],\n",
            "        [-0.4970, -0.6340, -0.4136, -0.9061],\n",
            "        [ 0.0622, -0.3107,  0.5125, -0.3027],\n",
            "        [-0.5540,  0.1259, -0.6217, -0.2455]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-1.5554e+06,  1.2175e+05, -1.0162e+05,  1.1929e+06],\n",
            "        [ 8.5449e+04,  9.4374e+03,  1.5003e+04, -6.1078e+04],\n",
            "        [-1.6558e+03, -9.4138e+02, -5.2210e+02,  8.8564e+02],\n",
            "        [-9.5424e+03,  9.6991e+02, -1.3002e+03,  6.7090e+03]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-1.1319,  0.2694, -0.3852,  0.8161],\n",
            "        [ 1.3536,  0.5741, -0.7379, -0.6644],\n",
            "        [-0.2808, -0.7499,  0.0888,  0.7762],\n",
            "        [ 0.0896, -1.1173,  0.3571,  0.1190]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.0050e-01, -5.0119e-02,  1.4675e-01, -1.1186e+00],\n",
            "        [-1.5010e-01, -4.3460e-01,  3.6793e-01, -6.2348e-01],\n",
            "        [ 6.5978e-01, -1.1218e-03, -1.6518e+00, -4.3579e-01],\n",
            "        [-3.5949e-01, -1.6683e+00,  5.0370e-01,  7.2137e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.1290, -0.1254, -0.3349, -0.1467],\n",
            "        [ 0.4777,  0.9605, -0.9539, -0.3440],\n",
            "        [-0.6540,  0.8731, -0.8678, -0.3630],\n",
            "        [ 0.3261, -0.8035,  1.4921, -0.5295]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.7144,  0.0143, -0.0224,  0.5395],\n",
            "        [ 0.2556,  0.2662,  1.8615,  0.8922],\n",
            "        [-0.1130, -0.1486, -0.8376,  0.5619],\n",
            "        [-0.3793, -0.2062, -0.4119,  0.7810]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.2349e+00, -1.9844e+00, -1.6628e+00,  1.2129e+00],\n",
            "        [-4.3133e+00, -1.5642e+01, -9.7711e+00,  2.3067e+00],\n",
            "        [-6.5962e+00,  9.6135e+00, -7.1046e+00,  5.9598e+00],\n",
            "        [-1.1093e+01, -6.2528e-03,  7.8865e+00,  3.7992e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[ 0.9504, -4.4494, -0.9063, -0.1275],\n",
            "        [ 1.0268, -2.6385, -0.8795, -2.0588],\n",
            "        [-3.4205,  3.2203, -2.4560, -0.6394],\n",
            "        [-3.3374,  2.5235, -1.2875,  3.3008]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.9335, -0.4323, -1.0015, -1.3417],\n",
            "        [-1.7699, -0.4362, -0.6186,  0.9132],\n",
            "        [-0.5699, -0.8665, -4.1754, -2.6145],\n",
            "        [-0.3008, -0.1579, -0.8499,  0.0956]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [40/14435 (0%)]\tLoss: 177513.921875\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-1.3070,  0.4019, -1.1242, -0.4151],\n",
            "        [-1.6317, -0.3058, -0.9671, -0.6447],\n",
            "        [-1.6681,  0.1881, -0.1872, -0.0189],\n",
            "        [-2.0125,  0.3692, -0.9391, -1.4587]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-2.8473, -0.8371, -1.6162, -0.4052],\n",
            "        [-1.4314, -0.0871, -1.3734, -1.5644],\n",
            "        [-2.7131, -0.1395, -2.0114,  0.0923],\n",
            "        [-1.1052, -1.3527, -0.8568, -2.6285]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "tensor([[-0.5803,  0.9366, -1.5389, -3.8533],\n",
            "        [-4.0977, -0.7072, -2.2455, -0.4605],\n",
            "        [-2.9990, -1.1401, -1.6584, -2.0465],\n",
            "        [-2.3112,  1.0908, -3.1930, -0.3089]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.0915,  0.1243, -4.4731, -3.0517],\n",
            "        [-5.6633, -1.9247, -3.4221, -3.1130],\n",
            "        [-5.2394,  0.7533, -2.9748, -3.0070],\n",
            "        [-5.0334,  0.6203, -1.2408,  0.4679]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-1.5831, -1.1470, -3.4770, -3.9845],\n",
            "        [-2.6057, -3.6914, -3.5971, -2.9574],\n",
            "        [-3.5163, -3.2783, -3.0235, -3.9233],\n",
            "        [-4.8719, -2.0793, -1.4851, -2.9514]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-3.1211, -0.6366,  0.5595, -1.7696],\n",
            "        [-2.0752, -1.1277, -1.3596, -2.0484],\n",
            "        [-2.2051, -0.3513, -0.4876, -1.8826],\n",
            "        [-2.5898, -0.2259, -0.8649, -1.5294]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "tensor([[-1.5588,  0.3586, -0.4386, -1.8820],\n",
            "        [-2.3060, -1.0130,  0.1698, -1.9822],\n",
            "        [-1.1403,  0.3187, -1.4309, -0.4792],\n",
            "        [-2.4844, -0.6005, -0.9189, -1.2788]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-3.4199, -0.9778, -0.8401, -2.9032],\n",
            "        [-2.7415, -1.4245, -1.0818, -1.5253],\n",
            "        [-3.1850, -1.6918, -0.8849, -2.9584],\n",
            "        [-2.1511,  0.2735,  0.3691, -2.2186]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-3.7384, -1.5887, -0.3360, -3.2806],\n",
            "        [-3.0470, -0.2839, -0.1849, -2.8113],\n",
            "        [-4.0544, -1.6776,  0.4043, -1.8526],\n",
            "        [-3.5439, -0.6774,  0.2254, -3.0118]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.5420, -0.9944, -0.4662, -2.1836],\n",
            "        [-2.2675, -0.2256, -1.1425, -2.5022],\n",
            "        [-3.7170, -0.8589,  0.2717, -1.8486],\n",
            "        [-3.0209, -0.8112, -0.0777, -1.4516]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [80/14435 (1%)]\tLoss: 158731.609375\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.0280, -0.3029, -1.8967, -0.4187],\n",
            "        [-1.0725,  0.3444, -0.4100, -0.5515],\n",
            "        [ 0.0294, -0.8034,  0.3269, -0.7370],\n",
            "        [-0.4757, -0.6182, -0.9289, -0.3327]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.4934,  0.0559, -1.4639, -1.6572],\n",
            "        [-1.3344,  0.1199, -0.3379, -0.6002],\n",
            "        [-1.0450, -0.4633, -0.9511, -0.3608],\n",
            "        [-0.4644, -0.6069, -0.1348, -0.3880]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.0443, -0.1082, -1.0297, -0.7892],\n",
            "        [-1.8471, -0.6356, -0.7569, -0.6923],\n",
            "        [-2.2644, -0.5698, -1.0213, -1.4851],\n",
            "        [-1.8260, -1.4177, -1.3249, -1.8107]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-2.1474,  0.7587, -1.2257, -1.1285],\n",
            "        [-2.7194, -1.0089, -2.1727, -2.3369],\n",
            "        [-2.8056, -1.3918, -1.4218, -1.8885],\n",
            "        [-1.9879,  0.2105, -1.1188, -1.4662]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-2.7184,  0.0607, -0.9112, -0.2114],\n",
            "        [-1.4636,  0.1108, -0.4211, -0.6774],\n",
            "        [-1.4912, -0.2334, -1.2280, -0.5952],\n",
            "        [-1.8169, -0.4600, -1.6573, -2.1168]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.3941, -0.6110, -0.4971, -0.6139],\n",
            "        [-0.4243,  0.6088, -0.7581, -1.3929],\n",
            "        [-2.7654,  0.0514, -0.2226, -0.9520],\n",
            "        [-1.2324, -0.5074,  0.1121, -0.0725]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-3.0800,  0.8199, -1.4924, -1.1116],\n",
            "        [-4.3476, -0.0149, -1.3885, -1.9602],\n",
            "        [-4.1499, -0.6146, -2.1456, -0.5283],\n",
            "        [-4.3737,  0.4187, -1.7371, -2.1404]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-5.3619, -0.9216, -1.7775, -1.7398],\n",
            "        [-4.9873,  0.2124, -2.1608, -1.7890],\n",
            "        [-4.3246, -0.3654, -0.6394, -1.5400],\n",
            "        [-4.6675, -0.2399, -1.3458, -1.3644]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-4.4443, -1.2655, -2.1397, -2.0910],\n",
            "        [-4.8804, -0.8599, -1.0233, -1.4960],\n",
            "        [-4.6402, -1.5090, -2.0348, -1.0373],\n",
            "        [-2.8345,  0.1156, -0.8840, -2.3167]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-0.6713, -0.0399, -0.8770, -1.4103],\n",
            "        [-2.2018, -0.6013, -0.5985, -1.6706],\n",
            "        [-1.2833, -0.7332, -1.0603, -0.5459],\n",
            "        [-2.4936, -0.2268, -0.8808, -0.3373]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [120/14435 (1%)]\tLoss: 156420.296875\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.5345,  0.2021, -0.4575,  0.5119],\n",
            "        [-0.5776,  0.1093,  0.5025, -0.3401],\n",
            "        [-1.4870, -0.1486, -2.1099, -0.7913],\n",
            "        [-1.9077, -0.3553,  0.1567, -1.9979]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.8720, -0.6780, -1.0685, -0.4112],\n",
            "        [-2.3838, -0.6555, -1.8160,  0.0062],\n",
            "        [-1.4969,  0.1149, -0.4358, -0.4078],\n",
            "        [-3.0983, -0.6022, -0.9710, -0.4334]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.4465, -0.6273, -1.8688,  0.1808],\n",
            "        [-2.1549, -1.4308, -1.8888, -0.3911],\n",
            "        [-1.9717, -0.5305, -2.3606, -0.9481],\n",
            "        [-2.9644, -1.2545, -1.0636, -1.0173]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-1.4687, -0.3018, -1.6275, -0.6809],\n",
            "        [-1.7314,  0.1377, -0.2072, -0.9536],\n",
            "        [-1.5063, -0.5492, -1.3392, -0.7253],\n",
            "        [-0.9946, -0.7380, -0.5123, -0.3200]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.7148, -0.1740, -0.3924,  0.6665],\n",
            "        [-1.1627, -0.9966, -0.8371, -0.4483],\n",
            "        [-1.5576, -0.7259, -0.1204, -0.6733],\n",
            "        [-1.4227,  0.2574,  0.1734, -0.9515]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-0.6565,  0.5116, -0.2841,  0.1721],\n",
            "        [-1.0964,  0.5784,  0.8398, -1.5726],\n",
            "        [-1.2525, -0.2824, -1.4434, -1.1910],\n",
            "        [-0.4509,  0.8016,  0.1087, -0.0249]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.9797, -0.0095, -1.9319, -1.9542],\n",
            "        [-2.0938,  0.3024, -1.5199, -1.6673],\n",
            "        [-1.8870, -0.0359, -1.2365, -1.2717],\n",
            "        [-2.5693,  0.7277, -0.1361, -0.4445]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.1933,  0.5681, -0.3091, -0.5863],\n",
            "        [-1.1682,  0.6732, -1.1820, -1.4189],\n",
            "        [-2.0020,  0.8676, -0.6743,  0.0278],\n",
            "        [-1.1820, -0.7817, -1.0070, -1.5337]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.3672,  0.2415, -0.3024, -1.8918],\n",
            "        [-0.8370,  0.9006, -0.8732, -1.2246],\n",
            "        [-1.3764, -0.1559, -0.4451, -1.2512],\n",
            "        [-1.4651, -0.6884, -0.9205, -1.9568]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-1.8860,  0.1585, -0.4986,  0.0170],\n",
            "        [-1.2654,  0.4027,  0.2357,  0.0408],\n",
            "        [-2.6482,  0.3026,  0.4378, -1.5427],\n",
            "        [-1.2394, -0.4205, -0.5491, -1.2149]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [160/14435 (1%)]\tLoss: 155418.750000\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-1.8313,  1.0054, -1.7624, -0.2844],\n",
            "        [-2.5789,  1.0358, -0.7875, -1.5221],\n",
            "        [-2.4804, -0.3837, -0.3754, -0.1865],\n",
            "        [-1.2937, -0.2098, -0.1190, -0.4324]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0.]])\n",
            "tensor([[-2.3003,  0.4172, -1.2467, -0.8798],\n",
            "        [-1.8626,  1.1277, -1.3001,  0.5468],\n",
            "        [-2.1363,  0.7871, -1.2371,  0.1681],\n",
            "        [-2.2882,  0.2190,  0.0743, -1.3586]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-2.0238,  0.7777, -0.5641, -1.1780],\n",
            "        [-1.5715, -0.0660, -0.0840, -1.4336],\n",
            "        [-1.7679, -0.4174, -0.4623, -0.7024],\n",
            "        [-1.4168, -0.2393, -0.2962, -0.4630]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-0.7051,  0.4014, -0.8480, -0.3850],\n",
            "        [-1.7003,  0.3947, -0.0585, -0.3486],\n",
            "        [-1.6692,  0.5667,  0.4101, -0.7560],\n",
            "        [-1.2389,  0.8490, -0.9572,  0.2183]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.1416,  0.8662, -1.5219, -0.2007],\n",
            "        [-1.6030, -0.6017, -0.2009, -0.0530],\n",
            "        [-1.2512,  0.1111, -0.3238, -0.4556],\n",
            "        [-1.3709, -0.3156, -0.2240, -0.8434]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[-0.2080, -0.3198, -0.3717, -0.5868],\n",
            "        [-0.2297,  0.5289, -0.3835, -0.0667],\n",
            "        [-2.0506, -0.1552, -0.0673,  0.4636],\n",
            "        [-1.0315,  0.4966, -0.2743, -1.1102]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.4502,  0.7463, -0.8584, -0.8946],\n",
            "        [-1.0330,  0.1207, -0.2837,  0.4107],\n",
            "        [-0.4776,  0.1209,  0.1842,  0.1204],\n",
            "        [-1.0500, -0.3268, -0.7101,  0.4175]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "tensor([[-1.2193,  0.6773, -0.0451,  0.9683],\n",
            "        [-1.1098,  1.3532, -1.1026,  1.0095],\n",
            "        [-1.9154, -0.0425,  0.0086,  0.0623],\n",
            "        [-1.2755, -0.2144, -1.0263,  0.4855]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-2.4605,  0.1145, -0.4920, -0.1615],\n",
            "        [-2.2829,  0.9607, -1.0547,  0.2062],\n",
            "        [-1.7815,  0.0493, -0.2328, -0.3132],\n",
            "        [-1.8107, -0.1519, -0.9953, -0.8632]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.4576, -1.0104, -0.0869,  0.0593],\n",
            "        [-1.4357,  0.3779, -0.1041, -0.3106],\n",
            "        [-1.4236,  0.3805, -0.6107, -0.2291],\n",
            "        [-1.0304,  0.6704, -1.0057,  0.3543]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "Train Epoch: 0 [200/14435 (1%)]\tLoss: 153225.156250\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-1.1943, -0.2045,  0.1456,  0.4022],\n",
            "        [-1.9284,  0.2123,  0.3252,  0.1189],\n",
            "        [-1.0929,  0.1738, -0.1211, -0.2393],\n",
            "        [-2.0473,  0.3254, -0.1259,  1.0115]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.5626, -0.1093, -0.7376,  0.3927],\n",
            "        [-0.6971,  1.0227, -1.1056,  0.0087],\n",
            "        [-1.5109, -0.5302, -0.7098, -0.5173],\n",
            "        [-1.5396, -0.7295, -1.1840, -0.6804]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.7317,  0.3474, -0.7104, -0.5795],\n",
            "        [-1.1910,  1.4160, -1.7795,  0.8513],\n",
            "        [-2.0266, -0.4796, -1.1565,  0.5999],\n",
            "        [-1.3033,  0.9841,  0.0881, -0.0745]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-1.4957,  0.1542, -0.0938, -1.7288],\n",
            "        [-1.6357,  1.0400, -0.2904,  0.1389],\n",
            "        [-0.4570,  0.9315, -0.7509,  0.0927],\n",
            "        [-1.0312,  0.5112, -1.0595,  0.4067]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.3217, -0.2314,  0.4793, -0.5768],\n",
            "        [-1.3589,  0.1031, -0.3685,  0.3440],\n",
            "        [-0.4953,  1.6792, -0.7937, -0.6603],\n",
            "        [-1.6930, -0.0361, -0.6471,  0.1316]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[-2.0195, -0.2471, -0.6570, -1.2469],\n",
            "        [-1.9333,  0.2709,  0.2687, -0.1265],\n",
            "        [-0.9077, -0.0165, -0.6135, -0.7792],\n",
            "        [-1.2995,  0.4639, -0.2008, -0.2310]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.5028,  0.6719, -0.6121, -0.6630],\n",
            "        [-1.6147,  0.3107,  0.1835, -0.6156],\n",
            "        [-1.4756,  0.4583, -0.9338, -0.5323],\n",
            "        [-2.2483,  0.3398, -1.2885, -0.3020]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n",
            "x: torch.Size([4, 3, 224, 224])\n",
            "x_hat: torch.Size([4, 3, 16, 16])\n",
            "z: torch.Size([4, 256])\n",
            "y: torch.Size([4, 4])\n",
            "tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.]])\n",
            "tensor([[-2.0726,  0.2678, -0.0085, -0.7606],\n",
            "        [-1.1448,  0.3760, -0.6799, -1.7110],\n",
            "        [-2.0369,  1.3088, -0.6645, -0.6783],\n",
            "        [-1.0358,  0.3512, -0.1497, -0.9165]], grad_fn=<AddmmBackward0>)\n",
            "target torch.Size([4, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-d8ecce03c329>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-31d5b7108408>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKLD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----BACK UP------"
      ],
      "metadata": {
        "id": "Im6_Mph6dKke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla VAE from Git Repo"
      ],
      "metadata": {
        "id": "F3F1bR262JFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaVAE():\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 latent_dim: int,\n",
        "                 hidden_dims: List = None,\n",
        "                 **kwargs) -> None:\n",
        "        super(VanillaVAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        modules = []\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size= 3, stride= 2, padding  = 1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride = 2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                                               hidden_dims[-1],\n",
        "                                               kernel_size=3,\n",
        "                                               stride=2,\n",
        "                                               padding=1,\n",
        "                                               output_padding=1),\n",
        "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "                            nn.LeakyReLU(),\n",
        "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
        "                                      kernel_size= 3, padding= 1),\n",
        "                            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes.\n",
        "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
        "        :return: (Tensor) List of latent codes\n",
        "        \"\"\"\n",
        "        result = self.encoder(input)\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "\n",
        "        # Split the result into mu and var components\n",
        "        # of the latent Gaussian distribution\n",
        "        mu = self.fc_mu(result)\n",
        "        log_var = self.fc_var(result)\n",
        "\n",
        "        return [mu, log_var]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Maps the given latent codes\n",
        "        onto the image space.\n",
        "        :param z: (Tensor) [B x D]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, 2, 2)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) from\n",
        "        N(0,1).\n",
        "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
        "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
        "        :return: (Tensor) [B x D]\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
        "        mu, log_var = self.encode(input)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return  [self.decode(z), input, mu, log_var]\n",
        "\n",
        "    def loss_function(self,\n",
        "                      *args,\n",
        "                      **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        Computes the VAE loss function.\n",
        "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
        "        :param args:\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        recons = args[0]\n",
        "        input = args[1]\n",
        "        mu = args[2]\n",
        "        log_var = args[3]\n",
        "\n",
        "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
        "        recons_loss =F.mse_loss(recons, input)\n",
        "\n",
        "\n",
        "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "        loss = recons_loss + kld_weight * kld_loss\n",
        "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int,\n",
        "               current_device: int, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Samples from the latent space and return the corresponding\n",
        "        image space map.\n",
        "        :param num_samples: (Int) Number of samples\n",
        "        :param current_device: (Int) Device to run the model\n",
        "        :return: (Tensor)\n",
        "        \"\"\"\n",
        "        z = torch.randn(num_samples,\n",
        "                        self.latent_dim)\n",
        "\n",
        "        z = z.to(current_device)\n",
        "\n",
        "        samples = self.decode(z)\n",
        "        return samples\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Given an input image x, returns the reconstructed image\n",
        "        :param x: (Tensor) [B x C x H x W]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "\n",
        "        return self.forward(x)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "gYwX83-94BDg",
        "outputId": "98b7be90-0ebe-4920-cd52-d04f4a42d0fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3dd8e8909901>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVanillaVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3dd8e8909901>\u001b[0m in \u001b[0;36mVanillaVAE\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mlatent_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                  \u001b[0mhidden_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                  **kwargs) -> None:\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVanillaVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Classifier"
      ],
      "metadata": {
        "id": "-kK9iPKr2Ope"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAEClassifier(BaseVAE):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 num_classes: int,\n",
        "                 hidden_dims: List = None,\n",
        "                 **kwargs) -> None:\n",
        "        super(VAEClassifier, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        modules = []\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_logits = nn.Linear(hidden_dims[-1]*4, num_classes)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(num_classes, hidden_dims[-1] * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride=2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                               hidden_dims[-1],\n",
        "                               kernel_size=3,\n",
        "                               stride=2,\n",
        "                               padding=1,\n",
        "                               output_padding=1),\n",
        "            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(hidden_dims[-1], out_channels=3,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        result = self.encoder(input)\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "        logits = self.fc_logits(result)\n",
        "        return [logits, F.softmax(logits, dim=1)]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, 2, 2)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, logits: Tensor) -> Tensor:\n",
        "        std = torch.ones_like(logits)\n",
        "        eps = torch.randn_like(logits)\n",
        "        return eps * std + logits\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
        "        logits, probs = self.encode(input)\n",
        "        z = self.reparameterize(logits)\n",
        "        return [self.decode(z), input, probs]\n"
      ],
      "metadata": {
        "id": "6jEF17KW57ol"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from torch import Tensor\n",
        "\n",
        "class BaseVAE(nn.Module):\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        super(BaseVAE, self).__init__()\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, input: Tensor) -> Any:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *inputs: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
        "        pass"
      ],
      "metadata": {
        "id": "uP5-LBBv6UIz"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.2 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1da3383a1b58fe481baf23ca14731c20d0a972ba0c6221953a6c3e65df3165e3"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}