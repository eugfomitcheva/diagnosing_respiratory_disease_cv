{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Organizing Data #"
      ],
      "metadata": {
        "id": "O6psw7ckHCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kri_iQHdHRZc",
        "outputId": "ad0f2f5c-ec6b-41a4-8640-43b8222fcf6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't need to re-run"
      ],
      "metadata": {
        "id": "Q1PDJwKzbowB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "# Set paths to image folders\n",
        "class1_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Viral_Pneumonia/images'\n",
        "class2_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Normal/images'\n",
        "class3_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/Lung_Opacity/images'\n",
        "class4_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/COVID/images'\n",
        "\n",
        "# Set paths to output directories\n",
        "train_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/val'\n",
        "test_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/test'\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Split images into train, validation, and test sets\n",
        "for class_dir, class_name in zip([class1_dir, class2_dir, class3_dir, class4_dir], ['class1', 'class2', 'class3', 'class4']):\n",
        "    image_files = os.listdir(class_dir)\n",
        "    train_files, test_files = train_test_split(image_files, test_size=0.1, random_state=42)\n",
        "    train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Copy train images to train folder\n",
        "    for file_name in train_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(train_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n",
        "\n",
        "    # Copy validation images to validation folder\n",
        "    for file_name in val_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(val_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n",
        "\n",
        "    # Copy test images to test folder\n",
        "    for file_name in test_files:\n",
        "        src_path = os.path.join(class_dir, file_name)\n",
        "        dst_path = os.path.join(test_dir, class_name, file_name)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        copyfile(src_path, dst_path)\n"
      ],
      "metadata": {
        "id": "ohY784FXHCJQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzyVsKjhi0v0"
      },
      "source": [
        "# Advanced Models: EVAE-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IL3egfh8i0v3"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.models as models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kAJB8cBKi0v5"
      },
      "outputs": [],
      "source": [
        "class EVAE(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super(EVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes #4\n",
        "        \n",
        "        # Define ResNet50 Encoder\n",
        "        resnet = models18(pretrained=True)\n",
        "        resnet_layers = list(resnet.children())[:-1]  # Remove last layer (classification head)\n",
        "        self.resnet_encoder = nn.Sequential(*resnet_layers)\n",
        "        \n",
        "        # Define VGG16 Encoder\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        vgg16_layers = list(vgg16.features.children())[:-1]  # Remove last layer (max pooling)\n",
        "        self.vgg16_encoder = nn.Sequential(*vgg16_layers)\n",
        "        \n",
        "        # Define reparameterization layers\n",
        "        self.fc1 = nn.Linear(4096, latent_dim)  # 4096 = 2048 from ResNet50 + 2048 from VGG16\n",
        "        self.fc2 = nn.Linear(4096, latent_dim)\n",
        "        \n",
        "        # Define classification head\n",
        "        self.classification_head = nn.Linear(latent_dim, num_classes)\n",
        "        \n",
        "        # Define decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def encode(self, x):\n",
        "        resnet_features = self.resnet_encoder(x)\n",
        "        vgg16_features = self.vgg16_encoder(x)\n",
        "        features = torch.cat((resnet_features.view(resnet_features.size(0), -1), \n",
        "                              vgg16_features.view(vgg16_features.size(0), -1)), dim=1)\n",
        "        h1 = F.relu(self.fc1(features))\n",
        "        h2 = F.relu(self.fc2(features))\n",
        "        return h1, h2\n",
        "    \n",
        "    def decode(self, z):\n",
        "        x_hat = self.decoder(z.unsqueeze(-1).unsqueeze(-1))\n",
        "        return x_hat\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps * std + mu\n",
        "        x_hat = self.decode(z)\n",
        "        y = self.classification_head(z)\n",
        "        return x_hat, y, mu, log_var\n",
        "    \n",
        "    def loss_function(self, x_hat, x, y, target, mu, log_var):\n",
        "        BCE = F.binary_cross_entropy(x_hat.view(-1, 1024*1024), x.view(-1, 1024*1024), reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        Lcls = F.cross_entropy(y, target, reduction='sum')\n",
        "        return BCE, KLD, Lcls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "0wFHZC0odaAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms # need to adapt image format\n",
        "\n",
        "\n",
        "# Access train data\n",
        "train_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/train'\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
        "train_data = datasets.ImageFolder(train_dir, transform=transform)"
      ],
      "metadata": {
        "id": "RWtgOVgm-ovV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, y, mu, log_var = model(data)\n",
        "        BCE, KLD, Lcls = model.loss_function(recon_batch, data, y, mu, log_var)\n",
        "        loss = BCE + KLD + Lcls\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "OV192UynYuQX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = EVAE(latent_dim=256, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(model, optimizer, train_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "w6Sz5yncY01h",
        "outputId": "bfd35bb9-60c5-43c3-b4c1-a94e3feeae52"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f91148dc1ef1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-32aa8d55d7c5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKLD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cc5d792fd3bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cc5d792fd3bf>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         features = torch.cat((resnet_features.view(resnet_features.size(0), -1), \n\u001b[1;32m     40\u001b[0m                               vgg16_features.view(vgg16_features.size(0), -1)), dim=1)\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x102400 and 4096x256)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----BACK UP------"
      ],
      "metadata": {
        "id": "Im6_Mph6dKke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla VAE from Git Repo"
      ],
      "metadata": {
        "id": "F3F1bR262JFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaVAE():\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 latent_dim: int,\n",
        "                 hidden_dims: List = None,\n",
        "                 **kwargs) -> None:\n",
        "        super(VanillaVAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        modules = []\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size= 3, stride= 2, padding  = 1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride = 2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                                               hidden_dims[-1],\n",
        "                                               kernel_size=3,\n",
        "                                               stride=2,\n",
        "                                               padding=1,\n",
        "                                               output_padding=1),\n",
        "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "                            nn.LeakyReLU(),\n",
        "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
        "                                      kernel_size= 3, padding= 1),\n",
        "                            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes.\n",
        "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
        "        :return: (Tensor) List of latent codes\n",
        "        \"\"\"\n",
        "        result = self.encoder(input)\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "\n",
        "        # Split the result into mu and var components\n",
        "        # of the latent Gaussian distribution\n",
        "        mu = self.fc_mu(result)\n",
        "        log_var = self.fc_var(result)\n",
        "\n",
        "        return [mu, log_var]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Maps the given latent codes\n",
        "        onto the image space.\n",
        "        :param z: (Tensor) [B x D]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, 2, 2)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) from\n",
        "        N(0,1).\n",
        "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
        "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
        "        :return: (Tensor) [B x D]\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
        "        mu, log_var = self.encode(input)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return  [self.decode(z), input, mu, log_var]\n",
        "\n",
        "    def loss_function(self,\n",
        "                      *args,\n",
        "                      **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        Computes the VAE loss function.\n",
        "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
        "        :param args:\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        recons = args[0]\n",
        "        input = args[1]\n",
        "        mu = args[2]\n",
        "        log_var = args[3]\n",
        "\n",
        "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
        "        recons_loss =F.mse_loss(recons, input)\n",
        "\n",
        "\n",
        "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "        loss = recons_loss + kld_weight * kld_loss\n",
        "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int,\n",
        "               current_device: int, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Samples from the latent space and return the corresponding\n",
        "        image space map.\n",
        "        :param num_samples: (Int) Number of samples\n",
        "        :param current_device: (Int) Device to run the model\n",
        "        :return: (Tensor)\n",
        "        \"\"\"\n",
        "        z = torch.randn(num_samples,\n",
        "                        self.latent_dim)\n",
        "\n",
        "        z = z.to(current_device)\n",
        "\n",
        "        samples = self.decode(z)\n",
        "        return samples\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Given an input image x, returns the reconstructed image\n",
        "        :param x: (Tensor) [B x C x H x W]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "\n",
        "        return self.forward(x)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "gYwX83-94BDg",
        "outputId": "98b7be90-0ebe-4920-cd52-d04f4a42d0fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3dd8e8909901>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVanillaVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3dd8e8909901>\u001b[0m in \u001b[0;36mVanillaVAE\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mlatent_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                  \u001b[0mhidden_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                  **kwargs) -> None:\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVanillaVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Classifier"
      ],
      "metadata": {
        "id": "-kK9iPKr2Ope"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAEClassifier(BaseVAE):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 num_classes: int,\n",
        "                 hidden_dims: List = None,\n",
        "                 **kwargs) -> None:\n",
        "        super(VAEClassifier, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        modules = []\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_logits = nn.Linear(hidden_dims[-1]*4, num_classes)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(num_classes, hidden_dims[-1] * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride=2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                               hidden_dims[-1],\n",
        "                               kernel_size=3,\n",
        "                               stride=2,\n",
        "                               padding=1,\n",
        "                               output_padding=1),\n",
        "            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(hidden_dims[-1], out_channels=3,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        result = self.encoder(input)\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "        logits = self.fc_logits(result)\n",
        "        return [logits, F.softmax(logits, dim=1)]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, 2, 2)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, logits: Tensor) -> Tensor:\n",
        "        std = torch.ones_like(logits)\n",
        "        eps = torch.randn_like(logits)\n",
        "        return eps * std + logits\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
        "        logits, probs = self.encode(input)\n",
        "        z = self.reparameterize(logits)\n",
        "        return [self.decode(z), input, probs]\n"
      ],
      "metadata": {
        "id": "6jEF17KW57ol"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "9XJjGzq_T-IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataloader for batching and shuffling the data\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "DmoglAXf-l1z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from torch import Tensor\n",
        "\n",
        "class BaseVAE(nn.Module):\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        super(BaseVAE, self).__init__()\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, input: Tensor) -> Any:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *inputs: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
        "        pass"
      ],
      "metadata": {
        "id": "uP5-LBBv6UIz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process to fixed size 224 x 224\n"
      ],
      "metadata": {
        "id": "AZIV3LYpa0Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate VAE model\n",
        "model = VAEClassifier(in_channels=3, num_classes=4, hidden_dims=[32, 64, 128, 256])\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "# Train model for specified number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Iterate over batches of training data\n",
        "    for images, labels in train_loader:\n",
        "        # Zero out gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through model\n",
        "        images = images.view(images.size(0), -1) # flatten\n",
        "        recon_images, mu, log_var = model(images)\n",
        "\n",
        "        # Compute reconstruction loss\n",
        "        loss = criterion(recon_images, images)\n",
        "\n",
        "        # Compute KL divergence loss\n",
        "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        loss += kl_loss\n",
        "\n",
        "        # Backward pass through model and update parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "450JEZMlAGu3",
        "outputId": "a82d4975-f8db-4111-87f9-4936db4ac9ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b72b3b534c38>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Forward pass through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrecon_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-026ae032277f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-026ae032277f>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 268203]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "l2mbci-4ctz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = EVAE(latent_dim=256, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        x, target = x.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        x_hat, y, mu, log_var = model(x)\n",
        "        bce_loss, kld_loss, cls_loss = model.loss_function(x_hat, x, y, target, mu, log_var)\n",
        "        loss = bce_loss + kld_loss + cls_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # with torch.no_grad():\n",
        "    #     for batch_idx, (x, target) in enumerate(val_loader):\n",
        "    #         x, target = x.to(device), target.to(device)\n",
        "    #         x_hat, y, mu, log_var = model(x)\n",
        "    #         bce_loss, kld_loss, cls_loss = model.loss_function(x_hat, x, y, target, mu, log_var)\n",
        "    #         loss = bce_loss + kld_loss + cls_loss\n",
        "    #         val_loss += loss.item()\n",
        "    #         _, predicted = torch.max(y.data, 1)\n",
        "    #         total += target.size(0)\n",
        "    #         correct += (predicted == target).sum().item()\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    # val_loss /= len(val_loader.dataset)\n",
        "    accuracy = 100 * correct / total\n",
        "    \n",
        "    print('Epoch: {}, Train Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, train_loss, accuracy))\n"
      ],
      "metadata": {
        "id": "81h99TdGUjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hArNT2Gdi0v8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class EVAE(nn.Module):\n",
        "    def init(self, input_dim, latent_dim, hidden_dims):\n",
        "        super().init()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.encoder = self.build_encoder()\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters())\n",
        "    \n",
        "def build_encoder(self):\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(self.input_dim, self.hidden_dims[0]))\n",
        "    layers.append(nn.ReLU())\n",
        "    for i in range(1, len(self.hidden_dims)):\n",
        "        layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.Linear(self.hidden_dims[-1], self.latent_dim))\n",
        "    self.classifier = nn.Linear(self.latent_dim, self.num_classes)\n",
        "    layers.append(nn.Linear(self.hidden_dims[-1], self.latent_dim))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def build_decoder(self):\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(self.latent_dim, self.hidden_dims[-1]))\n",
        "    layers.append(nn.ReLU())\n",
        "    for i in range(len(self.hidden_dims)-2, -1, -1):\n",
        "        layers.append(nn.Linear(self.hidden_dims[i+1], self.hidden_dims[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.Linear(self.hidden_dims[0], self.input_dim))\n",
        "    layers.append(nn.Sigmoid())\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def sampling(self, z_mean, z_log_var):\n",
        "    epsilon = torch.randn_like(z_mean)\n",
        "    return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def forward(self, x):\n",
        "    z_mean, z_log_var = self.encoder(x)\n",
        "    z = self.sampling(z_mean, z_log_var)\n",
        "    class_probs = F.softmax(self.classifier(z), dim=1)\n",
        "    x_pred = self.decoder(z)\n",
        "    return x_pred, z_mean, z_log_var, class_probs\n",
        "\n",
        "def vae_loss(self, x, x_pred, z_mean, z_log_var):\n",
        "    recon_loss = F.binary_cross_entropy(x_pred, x, reduction='sum')\n",
        "    kl_loss = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
        "    return recon_loss + kl_loss\n",
        "\n",
        "def train(self, x_train, x_val=None, batch_size=32, epochs=5):\n",
        "    train_dataset = TensorDataset(torch.Tensor(x_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    if x_val is not None:\n",
        "        val_dataset = TensorDataset(torch.Tensor(x_val))\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    else:\n",
        "        val_loader = None\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        for x_batch in train_loader:\n",
        "            x_batch = x_batch[0]\n",
        "            self.optimizer.zero_grad()\n",
        "            x_pred, z_mean, z_log_var = self.forward(x_batch)\n",
        "            loss = self.vae_loss(x_batch, x_pred, z_mean, z_log_var)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        if val_loader is not None:\n",
        "            with torch.no_grad():\n",
        "                for x_batch in val_loader:\n",
        "                    x_batch = x_batch[0]\n",
        "                    x_pred, z_mean, z_log_var = self.forward(x_batch)\n",
        "                    loss = self.vae_loss(x_batch, x_pred, z_mean, z_log_var)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= len(val_loader.dataset)\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCPVYWi-i0v7"
      },
      "outputs": [],
      "source": [
        "# EVAE-Net with pre-trained ResNet50 \n",
        "import tensorflow as tf\n",
        "\n",
        "# We implement EVAE-Net which concatenates the outputs of ResNet50 and VGG16 as the input of the encoder.\n",
        "\n",
        "class EVAE:\n",
        "    def __init__(self, input_dim, latent_dim, hidden_dims):\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.encoder = self.build_encoder()\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        \n",
        "    def build_encoder(self):\n",
        "        inputs = tf.keras.layers.Input(shape=(self.input_dim,))\n",
        "        x = inputs\n",
        "        for hidden_dim in self.hidden_dims:\n",
        "            x = tf.keras.layers.Dense(hidden_dim, activation='relu')(x)\n",
        "        z_mean = tf.keras.layers.Dense(self.latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(self.latent_dim)(x)\n",
        "        z = tf.keras.layers.Lambda(self.sampling)([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "    def build_decoder(self):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
        "        x = latent_inputs\n",
        "        for hidden_dim in reversed(self.hidden_dims):\n",
        "            x = tf.keras.layers.Dense(hidden_dim, activation='relu')(x)\n",
        "        outputs = tf.keras.layers.Dense(self.input_dim, activation='sigmoid')(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], self.latent_dim))\n",
        "        return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def vae_loss(self, x, x_pred, z_mean, z_log_var):\n",
        "        recon_loss = tf.keras.losses.binary_crossentropy(x, x_pred)\n",
        "        kl_loss = -0.5 * tf.keras.backend.mean(1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var), axis=-1)\n",
        "        return tf.keras.backend.mean(recon_loss + kl_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(x)\n",
        "            x_pred = self.decoder(z)\n",
        "            loss = self.vae_loss(x, x_pred, z_mean, z_log_var)\n",
        "        gradients = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def train(self, x_train, x_val=None, batch_size=32, epochs=100):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(len(x_train)).batch(batch_size)\n",
        "        val_dataset = None\n",
        "        if x_val is not None:\n",
        "            val_dataset = tf.data.Dataset.from_tensor_slices(x_val).batch(batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = tf.keras.metrics.Mean()\n",
        "            val_loss = tf.keras.metrics.Mean()\n",
        "            for x_batch in train_dataset:\n",
        "                loss = self.train_step(x_batch)\n",
        "                train_loss(loss)\n",
        "            if val_dataset is not None:\n",
        "                for x_batch in val_dataset:\n",
        "                    z_mean, z_log_var, z = self.encoder(x_batch)\n",
        "                    x_pred = self.decoder(z)\n",
        "                    loss = self.vae_loss(x_batch, x_pred, z_mean, z_log_var)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.2 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1da3383a1b58fe481baf23ca14731c20d0a972ba0c6221953a6c3e65df3165e3"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}